{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import streamlit as st\n",
    "\n",
    "from bokeh.plotting import figure, output_file\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.palettes import Spectral\n",
    "from bokeh.palettes import Spectral6, Magma, Inferno\n",
    "from bokeh.themes import built_in_themes\n",
    "from bokeh.io import curdoc\n",
    "\n",
    "from datetime import date, timedelta\n",
    "from IPython import get_ipython\n",
    "from PIL import Image\n",
    "from streamlit import caching\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import streamlit.components.v1 as components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamlit.delta_generator.DeltaGenerator at 0x13565e6ed08>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.title('Elissa\\'s Angels Inc.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('img/eskwelabs_logo.jpg')\n",
    "st.sidebar.image(image, caption='', use_column_width=True)\n",
    "st.sidebar.markdown(\"<h1 style='text-align: center;margin-bottom:50px'>Eskwelabs Data Science Fellowship Cohort V</h1>\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "add_selectbox = st.sidebar.radio(\n",
    "    \"\",\n",
    "    (\"Introduction and Problem Statement\", \"List of Tools\", \"Data Sourcing\", \"Data Set\", \"Data Cleaning\", \n",
    "     \"Exploratory Data Analysis\", \"Pipeline\", \"Recommender Engine\", \n",
    "     \"Results\", \"Contributors\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_selectbox == 'Introduction and Problem Statement':\n",
    "    st.write('')\n",
    "    \n",
    "    st.subheader('Introduction')\n",
    "    st.write('-----------------------------------------------------------------------') \n",
    "    st.write('<b>BUSINESS OBJECTIVE:</b>', unsafe_allow_html=True)\n",
    "    st.write('Provide unique and actionable insights and strategies on how to boost streams of the artists they handle in the market')\n",
    "    \n",
    "    st.write('<b>CLIENT SCENARIO:</b>', unsafe_allow_html=True)\n",
    "    st.write(\"\"\"\n",
    "    Elissa's Angels is a record label based in Manila, Philippines. Their artists were previously performing well but are recently dropping from\n",
    "    the charts. They reached out to the Eskwelabs Data Science team to get help on how to get their artists back up in the charts. \n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    st.write(\"ARTIST ROSTER:\")\n",
    "    st.markdown(\"<ul>\"\\\n",
    "                \"<li>Darren Espanto</li>\"\\\n",
    "                \"<li>Clean Bandit</li>\"\\\n",
    "                \"<li>Troye Sivan</li>\"\\\n",
    "                \"<li>Juan Karlos</li>\"\\\n",
    "                \"<li>IV Of Spades</li>\"\\\n",
    "                \"</ul>\", unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    The team looked into collaborations as a possible way to improve the performance of their artists in terms of streams.\n",
    "    The chart below shows the comparison of collaboration vs non-collaboration tracks performance over the period of ~3 years:\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    image = Image.open('img/collab_noncollab.png').convert('RGB')\n",
    "    st.image(image, caption='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-65c99505100c>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-65c99505100c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    elif add_selectbox == 'Outline':\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "elif add_selectbox == 'Outline':\n",
    "    st.subheader('Outline')\n",
    "    st.write('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-29915e2bca99>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-29915e2bca99>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    elif add_selectbox == 'List of Tools':\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "elif add_selectbox == 'List of Tools':\n",
    "    st.subheader('List of Tools')\n",
    "    st.write('-----------------------------')\n",
    "    st.write('-----------------------------------------------------------------------') \n",
    "    image = Image.open('img/spotify.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "    image = Image.open('img/jupyter.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "    image = Image.open('img/pandas.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "    #image = Image.open('img/heroku.jpg').convert('RGB')\n",
    "    #st.image(image, caption='', width=150, height=50)\n",
    "    image = Image.open('img/streamlit.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "    #image = Image.open('img/bokeh.png').convert('RGB')\n",
    "    #st.image(image, caption='', width=300, height=150)\n",
    "    #image = Image.open('img/github.png').convert('RGB')\n",
    "    #st.image(image, caption='', width=300, height=150)\n",
    "    #image = Image.open('img/regex.jpeg').convert('RGB')\n",
    "    #st.image(image, caption='', width=300, height=150)\n",
    "    #image = Image.open('img/scipy.png').convert('RGB')\n",
    "    #st.image(image, caption='', width=300, height=150)\n",
    "    image = Image.open('img/seaborn.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "    image = Image.open('img/matplotlib.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "    image = Image.open('img/numpy.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Data Sourcing':\n",
    "    st.subheader('Data Sourcing')\n",
    "    st.write('-----------------------------')\n",
    "    \n",
    "    #st.write('<b></b>', unsafe_allow_html=True)\n",
    "    #st.markdown('<b>Data from January 2018 - September 2020</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    st.write(\"\"\"\n",
    "    The project used Spotify’s API data on the daily Top 200 charts in the Philippines from January 2018 to September 2020. Data was scraped instead of being manually downloaded to ease the data extraction process. The extracted data includes the tacks’ audio features such as key, mode, acousticness, danceability, energy, instrumentalness, liveness, loudness, speechiness, valence, and tempo.\n",
    "    For our clients' artist data, we extracted their entire discography from Spotify's API.\n",
    "\n",
    "    More information on the tracks’ audio features can be found here: https://developer.spotify.com/documentation/web-api/reference/tracks/get-audio-features/\n",
    "    \"\"\")\n",
    "    \n",
    "    DATA_URL = ('data/team_elissa_discography_tracks.csv')\n",
    "    \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data)\n",
    "    \n",
    "    df=data\n",
    "    \n",
    "    feature_cols = ['acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', \n",
    "                'speechiness', 'tempo', 'valence', 'popularity']\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    df['loudness'] = scaler.fit_transform(df[['loudness']])\n",
    "    df['tempo'] =  scaler.fit_transform(df[['tempo']])\n",
    "\n",
    "    tracks1_df = df[df['artist'] == \"Clean Bandit\"]\n",
    "\n",
    "    for col in ['popularity', 'danceability', 'energy',\n",
    "           'loudness', 'speechiness', 'acousticness', 'instrumentalness',\n",
    "           'liveness', 'valence', 'tempo']:\n",
    "        fig = plt.figure()\n",
    "        ax= fig.add_subplot(111)\n",
    "    \n",
    "        sns.distplot(tracks1_df[col], ax=ax, label= \"Clean Bandit\")\n",
    "        xc = tracks1_df[col].median()\n",
    "        plt.axvline(x=xc, color='red')\n",
    "    #sns.distplot(tracks2_df[col], ax=ax, label= KEYWORD2)\n",
    "    #plt.title(\"%s vs %s: %s \" % (KEYWORD1,KEYWORD2,col))\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend(frameon=False)\n",
    "        st.pyplot(fig)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Data Set':\n",
    "    st.subheader('Data Set')\n",
    "    st.write('-----------------------------')\n",
    "    \n",
    "    st.write('<b>SPOTIFY DAILY CHARTS TRACKS:</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>Data from January 2018 - September 2020:</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL = ('data/spotify_daily_charts.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data)\n",
    "    st.markdown('<b>Data Dimensions:</b> Rows: 197800 Columns: 6', unsafe_allow_html=True)\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    st.write('<b>SPOTIFY DAILY CHARTS TRACKS W/ FEATURES</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>Data from January 2018 - September 2020</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL2 = ('data/spotify_daily_charts_tracks.csv')\n",
    "    \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data2 = pd.read_csv(DATA_URL2, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data2.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data2\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data2 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data2)\n",
    "    st.markdown('<b>Data Dimensions:</b> Rows: 2292 Columns: 19', unsafe_allow_html=True)\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    st.write('<b>MERGED DATASET</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>Data from January 2018 - September 2020</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL3 = ('data/df_merged.csv')\n",
    "    \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data3 = pd.read_csv(DATA_URL3, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data3.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data3\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data3 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data3)\n",
    "    st.markdown('<b>Data Dimensions:</b> Rows: 197800 Columns: 23', unsafe_allow_html=True)\n",
    "    \n",
    "    ##################################################\n",
    "    \n",
    "    st.write('<b>CLIENT DSCOGRAPHY</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>Data from January 2018 - September 2020</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL4 = ('data/raw_client.csv')\n",
    "    \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data4 = pd.read_csv(DATA_URL4, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data4.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data4\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data4 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data4)\n",
    "    st.markdown('<b>Data Dimensions:</b> Rows: 59 Columns: 14', unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-4746d99fba15>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-4746d99fba15>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    elif add_selectbox == 'Data Cleaning':\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "elif add_selectbox == 'Data Cleaning':\n",
    "    st.subheader('Data Cleaning')\n",
    "    st.write('-----------------------------')\n",
    "    \n",
    "    st.write('<b>FINAL TRACKS DATASET:</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>Data from January 2018 - September 2020:</b>', unsafe_allow_html=True)\n",
    "    st.write('Inspired by the RFM model, we took the quartiles for streams and positions and added them together to derive a performance metric.', unsafe_allow_htm=True)\n",
    "    \n",
    "    DATA_URL = ('data/SumVal.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data)\n",
    "    st.markdown('<b>Data Dimensions:</b> Rows: 2292 Columns: 33', unsafe_allow_html=True)\n",
    "    \n",
    "    ################################################\n",
    "    \n",
    "    st.write('<b>FINAL CLIENT DATASET', unsafe_allow_html=True)\n",
    "    st.markdown('<b>Data from January 2018 - September 2020</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL2 = ('data/final_client_data.csv')\n",
    "    \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data2 = pd.read_csv(DATA_URL2, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data2.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data2\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data2 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data2)\n",
    "    st.markdown('<b>Data Dimensions:</b> Rows: 6 Columns: 13', unsafe_allow_html=True)\n",
    "    \n",
    "    #################################################\n",
    "    \n",
    "    st.write('<b>MODELLING FEATURES:</b>', unsafe_allow_html=True)\n",
    "    st.markdown(\"<ul>\"\\\n",
    "                \"<li>Duration</li>\"\\\n",
    "                \"<li>Acousticness</li>\"\\\n",
    "                \"<li>Danceability</li>\"\\\n",
    "                \"<li>Energy</li>\"\\\n",
    "                \"<li>Instrumentalness</li>\"\\\n",
    "                \"<li>Liveness</li>\"\\\n",
    "                \"<li>Loudness</li>\"\\\n",
    "                \"<li>Speechiness</li>\"\\\n",
    "                \"<li>Tempo</li>\"\\\n",
    "                 \"<li>Valence</li>\"\\\n",
    "                 \"<li>Mode</li>\"\\\n",
    "                 \"<li>Key</li>\"\\\n",
    "                \"</ul>\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Exploratory Data Analysis':\n",
    "    st.subheader('Exploratory Data Analysis')\n",
    "    st.write('-----------------------------')\n",
    "    \n",
    "    DATA_URL = ('data/merged_charts_tracks_data.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(df)\n",
    "    \n",
    "    merged = df\n",
    "    \n",
    "    merged = pd.read_csv('data/merged_charts_tracks_data.csv')\n",
    "\n",
    "    all_other_df = merged[merged['position'] < 51].groupby('date').mean()[['streams']] # Based on top 50 on charts\n",
    "#all_other_df\n",
    "\n",
    "\n",
    "    test = merged[merged['artist_name'] == \"Clean Bandit\"].groupby(\"track_name\").sum()[['streams']] \\\n",
    "        .sort_values(\"streams\", ascending = False)\n",
    "    test = test.reset_index()\n",
    "\n",
    "    clean_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for name in test['track_name']:\n",
    "        clean_df = clean_df.append(merged[merged['track_name'] == name])\n",
    "        #print(name)\n",
    "    \n",
    "    test = merged[merged['artist_name'] == \"Darren Espanto\"].groupby(\"track_id\").sum()[['streams']] \\\n",
    "        .sort_values(\"streams\", ascending = False)\n",
    "    test = test.reset_index()\n",
    "\n",
    "    darren_df = pd.DataFrame()\n",
    "\n",
    "    for name in test['track_id']:\n",
    "        darren_df = darren_df.append(merged[merged['track_id'] == name])\n",
    "        #print(name)\n",
    "    darren_df.groupby(\"track_name\").last()\n",
    "\n",
    "    test = merged[merged['artist_name'] == \"Troye Sivan\"].groupby(\"track_name\").sum()[['streams']] \\\n",
    "        .sort_values(\"streams\", ascending = False)\n",
    "    test = test.reset_index()\n",
    "\n",
    "    troye_df = pd.DataFrame()\n",
    "\n",
    "    for name in test['track_name']:\n",
    "        troye_df = troye_df.append(merged[merged['track_name'] == name])\n",
    "        #print(name)\n",
    "    troye_df.groupby(\"track_name\").last()\n",
    "\n",
    "    test = merged[merged['artist_name'] == \"juan karlos\"].groupby(\"track_name\").sum()[['streams']] \\\n",
    "        .sort_values(\"streams\", ascending = False)\n",
    "    test = test.reset_index()\n",
    "\n",
    "    juan_df = pd.DataFrame()\n",
    "\n",
    "    for name in test['track_name']:\n",
    "        juan_df = juan_df.append(merged[merged['track_name'] == name])\n",
    "        #print(name)\n",
    "    juan_df.groupby(\"track_name\").last()\n",
    "\n",
    "    test = merged[merged['artist_name'] == \"IV Of Spades\"].groupby(\"track_name\").sum()[['streams']] \\\n",
    "        .sort_values(\"streams\", ascending = False)\n",
    "    test = test.reset_index()\n",
    "\n",
    "    spades_df = pd.DataFrame()\n",
    "\n",
    "    for name in test['track_name']:\n",
    "        spades_df = spades_df.append(merged[merged['track_name'] == name])\n",
    "        #print(name)\n",
    "    spades_df.groupby(\"track_name\").last()\n",
    "\n",
    "\n",
    "\n",
    "    clean_df.groupby(\"track_name\").last()\n",
    "    all_other_df['clean'] =clean_df.groupby('date')['streams'].mean()\n",
    "    all_other_df['juan'] =juan_df.groupby('date')['streams'].mean()\n",
    "    all_other_df['darren'] =darren_df.groupby('date')['streams'].mean()\n",
    "    all_other_df['troye'] =troye_df.groupby('date')['streams'].mean()\n",
    "    all_other_df['spades'] =spades_df.groupby('date')['streams'].mean()\n",
    "\n",
    "    fig = plt.figure(figsize=(13,6))\n",
    "    ax = fig.add_subplot(111)\n",
    "#clean_df = clean_df.set_index('date')\n",
    "#juan_df = juan_df.set_index('date')\n",
    "\n",
    "#spades_df = spades_df.set_index('date')\n",
    "\n",
    "#troye_df = troye_df.set_index('date')\n",
    "#darren_df = darren_df.set_index('date')\n",
    "#all_other_df = merged.groupby('date').mean()['streams']\n",
    "\n",
    "    data1 = all_other_df['clean'].rolling(7).mean()\n",
    "    data2 =all_other_df['juan'].rolling(7).mean()\n",
    "    data3 = all_other_df['darren'].rolling(7).mean()\n",
    "    data4 = all_other_df['troye'].rolling(7).mean()\n",
    "    data5 = all_other_df['spades'].rolling(7).mean()\n",
    "\n",
    "    data6 = all_other_df['streams'].rolling(7).mean()\n",
    "\n",
    "\n",
    "\n",
    "    data6.plot(ax=ax, label='Mean of every song in dataset')\n",
    "\n",
    "    data1.plot(ax=ax, label='Clean Bandit songs')\n",
    "    data2.plot(ax=ax, label='juan karlos songs')\n",
    "    data3.plot(ax=ax, label='Darren Espanto songs')\n",
    "    data4.plot(ax=ax, label='Troye Sivan songs')\n",
    "    data5.plot(ax=ax, label='IV of Spades songs')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.ylabel('streams')\n",
    "    plt.title('Spotify Daily Streams')\n",
    "\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ##########################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "#import matplotlib.dates as mdates\n",
    "#ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "#get top position of all charting songs of the artist per day\n",
    "    data1 = spades_df.groupby('date')[['position']].min()\n",
    "#get rolling 7 day minimum of top daily positions\n",
    "    data1 = data1.rolling(7).min()\n",
    "    data2 = juan_df.groupby('date')[['position']].min()\n",
    "    data2 = data2.rolling(7).min()\n",
    "    data3 = darren_df.groupby('date')[['position']].min()\n",
    "#get rolling 7 day minimum of top daily positions\n",
    "    data3 = data3.rolling(7).min()\n",
    "    data4 = troye_df.groupby('date')[['position']].min()\n",
    "    data4 = data4.rolling(7).min()\n",
    "    data5 = clean_df.groupby('date')[['position']].min()\n",
    "#get rolling 7 day minimum of top daily positions\n",
    "    data5 = data5.rolling(7).min()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data1.plot(ax=ax, label='Clean Bandit')\n",
    "    data2.plot(ax=ax, label='XXXTENTACION')\n",
    "    data3.plot(ax=ax, label='Clean Bandit')\n",
    "    data4.plot(ax=ax, label='XXXTENTACION')\n",
    "    data5.plot(ax=ax, label='Clean Bandit')\n",
    "\n",
    "\n",
    "#reverse the yaxis to show 1 on top\n",
    "    plt.ylim([200,0])\n",
    "    plt.yticks([1]+np.arange(25,201,25).tolist())\n",
    "\n",
    "    L = plt.legend()\n",
    "    L.get_texts()[0].set_text('IV Of Spades')\n",
    "    L.get_texts()[1].set_text('juan karlos')\n",
    "    L.get_texts()[2].set_text('Darren Espanto')\n",
    "    L.get_texts()[3].set_text('Troye Sivan')\n",
    "    L.get_texts()[4].set_text('Clean Bandit')\n",
    "\n",
    "    plt.axhline(y=100.5, color='y', linestyle='-')\n",
    "#plt.show()\n",
    "    plt.ylabel('Chart Position')\n",
    "    plt.title('Spotify Weekly Top Chart Positions')\n",
    "#ax.set_xticklabels(data1.index)\n",
    "    date = [data1.index[0] , data1.index[120], data1.index[240] , data1.index[360], data1.index[480] , data1.index[600],\n",
    "       data1.index[720] , data1.index[953]]\n",
    "    ax.set_xticklabels(date)\n",
    "    \n",
    "    st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Client Track Classification':\n",
    "    st.subheader('Client Track Classification')\n",
    "    st.write('-----------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Recommender Engine':\n",
    "    st.subheader('Recommender Engine')\n",
    "    st.write('-----------------------------')\n",
    "    \n",
    "    st.write('<b>Clean Bandit Recommended Tracks:</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>___:</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL = ('data/clean_bandit_recommend_df.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data)\n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    st.write('<b>Darren Espanto Recommended Tracks:</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>____</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL1 = ('data/darren_espanto_recommend_df.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data1 = pd.read_csv(DATA_URL1, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data1.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data1\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data1 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data1)\n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    st.write('<b>IV Of Spades Recommended Tracks:</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>___</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL2 = ('data/iv_spades_recommend_df.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data2 = pd.read_csv(DATA_URL2, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data2.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data2\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data2 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data2)\n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    st.write('<b>Juan Karlos Recommended Tracks</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>___</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL3 = ('data/juan_carlos_recommend_df.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data3 = pd.read_csv(DATA_URL3, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data3.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data3\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data3 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data3)\n",
    "    \n",
    "    ##########################################################\n",
    "    \n",
    "    st.write('<b>Troye Sivan Recommended Tracks</b>', unsafe_allow_html=True)\n",
    "    st.markdown('<b>___</b>', unsafe_allow_html=True)\n",
    "    \n",
    "    DATA_URL4 = ('data/troye_sivan_recommend_df.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        data4 = pd.read_csv(DATA_URL4, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        data4.rename(lowercase, axis='columns', inplace=True)\n",
    "        return data4\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    data4 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    st.write(data4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Pipeline':\n",
    "    st.subheader('Model Pipeline')\n",
    "    st.write('-----------------------------')\n",
    "    \n",
    "    image = Image.open('img/pipeline.png').convert('RGB')\n",
    "    st.image(image, caption='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif add_selectbox == 'Results':\n",
    "    st.subheader('Results')\n",
    "    st.write('-----------------------------')\n",
    " \n",
    "    image = Image.open('img/model.png').convert('RGB')\n",
    "    st.image(image, caption='', width=300, height=150)\n",
    "\n",
    "    DATA_URL = ('data/clean_bar1.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df = pd.read_csv(DATA_URL, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    clean_bar = df\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(clean_bar['artist'], clean_bar['1'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    #############################################\n",
    "    \n",
    "    DATA_URL2 = ('data/clean_bar2.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df2 = pd.read_csv(DATA_URL2, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df2.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df2\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df2 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    clean_bar = df2\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(clean_bar['artist'], clean_bar['success_probability'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(\"Highest probability from entire dataset\") \n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    DATA_URL3 = ('data/juan_bar1.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df3 = pd.read_csv(DATA_URL3, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df3.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df3\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df3 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    juan_bar = df3\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(juan_bar['artist'], juan_bar['1'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    st.pyplot(fig)\n",
    "    ##\n",
    "    \n",
    "    DATA_URL4 = ('data/juan_bar2.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df4 = pd.read_csv(DATA_URL4, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df4.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df4\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df4 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    juan_bar = df4\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(juan_bar['artist'], juan_bar['success_probability'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(\"Highest probability from entire dataset\")\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    DATA_URL5 = ('data/troye_bar1.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df5 = pd.read_csv(DATA_URL5, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df5.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df5\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df5 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    troye_bar = df5\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(troye_bar['artist'], troye_bar['1'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    DATA_URL6 = ('data/troye_bar2.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df6 = pd.read_csv(DATA_URL6, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df6.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df6\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df6 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    troye_bar = df6\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(troye_bar['artist'], troye_bar['success_probability'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(\"Highest probability from entire dataset\") \n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    DATA_URL7 = ('data/darren_bar1.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df7 = pd.read_csv(DATA_URL7, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df7.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df7\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df7 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    darren_bar = df7\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(darren_bar['artist'], darren_bar['1'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    st.pyplot(fig)\n",
    "    ##\n",
    "    \n",
    "    DATA_URL8 = ('data/darren_bar2.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df8 = pd.read_csv(DATA_URL8, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df8.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df8\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df8 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    darren_bar = df8\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(darren_bar['artist'], darren_bar['success_probability'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(\"Highest probability from entire dataset\") \n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ##\n",
    "    \n",
    "    DATA_URL9 = ('data/ivos_bar1.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df9 = pd.read_csv(DATA_URL9, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df9.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df9\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df9 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    ivos_bar = df9\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(ivos_bar['artist'], ivos_bar['1'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    st.pyplot(fig)\n",
    "    ##\n",
    "    \n",
    "    DATA_URL10 = ('data/ivos_bar2.csv')\n",
    "        \n",
    "    @st.cache\n",
    "    def load_data(nrows):\n",
    "        df10 = pd.read_csv(DATA_URL10, nrows=nrows)\n",
    "        lowercase = lambda x: str(x).lower()\n",
    "        df10.rename(lowercase, axis='columns', inplace=True)\n",
    "        return df10\n",
    "    \n",
    "    # Create a text element and let the reader know the data is loading.\n",
    "    data_load_state = st.text('Loading data...')\n",
    "# Load 10,000 rows of data into the dataframe.\n",
    "    df10 = load_data(10000)\n",
    "# Notify the reader that the data was successfully loaded.\n",
    "    data_load_state.text(\"Done! (using st.cache)\")\n",
    "    \n",
    "    ivos_bar = df10\n",
    "    \n",
    "    fig = plt.figure(figsize=(13,4))\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.bar(ivos_bar['artist'], ivos_bar['success_probability'], width = 0.4)\n",
    "    plt.xlabel(\"Artists involved\") \n",
    "    plt.ylabel(\"Success Probability\") \n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.title(\"Highest probability from entire dataset\") \n",
    "    st.pyplot(fig)\n",
    "    \n",
    "    ##Embed Spotify Playlist \n",
    "#import streamlit.components.v1 as components\n",
    "   \n",
    "    st.write('Below is the tracks based on the results of our model.')\n",
    "    st.write('https://open.spotify.com/playlist/4tZSAyv6KxNDwrL5KSW5DZ')\n",
    "    components.html('<iframe src=\"https://open.spotify.com/embed/playlist/4tZSAyv6KxNDwrL5KSW5DZ\" allowtransparency=\"true\" width=\"800\" height=\"\" allow=\"encrypted-media\"></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    st.subheader('Contributors')\n",
    "    st.write('-----------------------------')\n",
    "    st.markdown(\"<ul>\"\\\n",
    "                \"<li>Alphonso Balagtas</li>\"\\\n",
    "                \"<li>Elissa Mae Cabal</li>\"\n",
    "                \"<li>Joleil Villena</li>\"\\\n",
    "                \"<li>Railenne Mae Ferrer </li>\"\\\n",
    "                \"<li>Roberto Bañadera Jr.</li>\"\\\n",
    "                 \"</ul>\", unsafe_allow_html=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
